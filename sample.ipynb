{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b40fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lmbmo\\.conda\\envs\\langraph_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import chromadb\n",
    "from docling.document_converter import DocumentConverter\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import ollama\n",
    "\n",
    "# Suppress some HuggingFace warnings for a cleaner notebook output\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53d26ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n",
      "Initializing ChromaDB...\n"
     ]
    }
   ],
   "source": [
    "# Defining the paths\n",
    "PDF_PATH = \"data/raw_pdfs/travel_and_reimbursement_policy.pdf\" \n",
    "DB_PATH = \"data/chromadb_store\"\n",
    "\n",
    "os.makedirs(os.path.dirname(PDF_PATH), exist_ok=True)\n",
    "os.makedirs(DB_PATH, exist_ok=True)\n",
    "\n",
    "# Initializing the embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\") # It outputs 384-dimensional vectors\n",
    "\n",
    "# Initialize ChromaDB\n",
    "print(\"Initializing ChromaDB...\")\n",
    "chroma_client = chromadb.PersistentClient(path=DB_PATH)\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"corporate_policies\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fa234",
   "metadata": {},
   "source": [
    "## The chunking strategy:\n",
    "\n",
    "1. **Parsing the PDF into Markdown**: Using the `Docling` library to read the PDF and convert it directly into Markdown format. By converting it to Markdown, you preserve the structural hierarchy of the corporate policy (e.g., # 1.0 Introduction, ## 1.1 Scope)\n",
    "\n",
    "2. **Semantic Chunking by Headers**: Uses LangChain to scan the file and split it every time it encounters an H1 (#), H2 (##), or H3 (###). This ensures that concepts stay together. If you have a section on \"Flight Reimbursements,\" all the text under that header is grouped into a single chunk. The MarkdownHeaderTextSplitter also extracts the headers and turns them into dictionary metadata. So, a chunk about economy class flights might automatically get the metadata: {\"Header_1\": \"Travel Policy\", \"Header_2\": \"Flights\", \"Header_3\": \"Class Types\"}.\n",
    "\n",
    "3. **The Fallback Splitter**: Semantic chunking is great, but what if ## General Rules is a massive wall of text that is 5,000 characters long? That would overwhelm the context window of your embedding model (BAAI/bge-small-en-v1.5), causing it to lose fidelity. This initializes a second splitter that breaks text into 600-character blocks, with a 100-character overlap between them. The fallback splitter acts as a safety net, taking those massive semantic sections and chopping them down into manageable, embeddable sizes while maintaining context via the 100-character overlap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d52f5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing PDF with Docling: data/raw_pdfs/travel_and_reimbursement_policy.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO] 2026-02-20 14:54:29,085 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-20 14:54:29,105 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\lmbmo\\.conda\\envs\\langraph_env\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-20 14:54:29,107 [RapidOCR] main.py:53: Using C:\\Users\\lmbmo\\.conda\\envs\\langraph_env\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_det_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-20 14:54:29,264 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-20 14:54:29,269 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\lmbmo\\.conda\\envs\\langraph_env\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-20 14:54:29,270 [RapidOCR] main.py:53: Using C:\\Users\\lmbmo\\.conda\\envs\\langraph_env\\Lib\\site-packages\\rapidocr\\models\\ch_ppocr_mobile_v2.0_cls_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-20 14:54:29,353 [RapidOCR] base.py:22: Using engine_name: onnxruntime\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-20 14:54:29,372 [RapidOCR] download_file.py:60: File exists and is valid: C:\\Users\\lmbmo\\.conda\\envs\\langraph_env\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n",
      "\u001b[32m[INFO] 2026-02-20 14:54:29,373 [RapidOCR] main.py:53: Using C:\\Users\\lmbmo\\.conda\\envs\\langraph_env\\Lib\\site-packages\\rapidocr\\models\\ch_PP-OCRv4_rec_infer.onnx\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text semantically by headers...\n",
      "Total chunks created: 289\n"
     ]
    }
   ],
   "source": [
    "print(f\"Parsing PDF with Docling: {PDF_PATH}\")\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(PDF_PATH)\n",
    "markdown_text = result.document.export_to_markdown()\n",
    "\n",
    "print(\"Chunking text semantically by headers...\")\n",
    "# Define the hierarchical structure to split on\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header_1\"),\n",
    "    (\"##\", \"Header_2\"),\n",
    "    (\"###\", \"Header_3\"),\n",
    "]\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on, strip_headers=False\n",
    ")\n",
    "header_splits = markdown_splitter.split_text(markdown_text)\n",
    "\n",
    "# Fallback for massive single sections\n",
    "fallback_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "\n",
    "chunks = []\n",
    "metadatas = []\n",
    "ids = []\n",
    "\n",
    "for i, split in enumerate(header_splits):\n",
    "    sub_chunks = fallback_splitter.split_text(split.page_content)\n",
    "    for j, sub_chunk in enumerate(sub_chunks):\n",
    "        chunks.append(sub_chunk)\n",
    "        \n",
    "        # Save the structural headers into the database metadata\n",
    "        meta = split.metadata.copy()\n",
    "        meta[\"source\"] = os.path.basename(PDF_PATH)\n",
    "        metadatas.append(meta)\n",
    "        \n",
    "        ids.append(f\"chunk_{i}_{j}\")\n",
    "\n",
    "print(f\"Total chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf130e",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78660b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting 5 Random Chunks ---\n",
      "\n",
      "Chunk ID : chunk_32_0\n",
      "Metadata : {'Header_2': 'Responsibility of Accounts Payable/ Business Office', 'source': 'travel_and_reimbursement_policy.pdf'}\n",
      "Content  :\n",
      "## Responsibility of Accounts Payable/ Business Office  \n",
      "The Accounts Payable Department and the Business Office are responsible for the following:  \n",
      "- Review of forms and attachments for completeness, accuracy, reasonableness and compliance with government regulations and University policies\n",
      "- Verification of required approved signatures\n",
      "- Ensuring proper tax treatment of taxable income items and compliance with IRS regulations\n",
      "============================================================\n",
      "\n",
      "Chunk ID : chunk_14_0\n",
      "Metadata : {'Header_2': 'Personal use of University-owned fleet vehicles is not permitted under any circumstances.', 'source': 'travel_and_reimbursement_policy.pdf'}\n",
      "Content  :\n",
      "## Personal use of University-owned fleet vehicles is not permitted under any circumstances.\n",
      "============================================================\n",
      "\n",
      "Chunk ID : chunk_42_2\n",
      "Metadata : {'Header_2': 'Special or Higher Risk Activities', 'source': 'travel_and_reimbursement_policy.pdf'}\n",
      "Content  :\n",
      "that expose a traveler to risk of injury or physical exertion should be outlined in the Student Release, Hold Harmless and Indemnification Form and signed by the participant.\n",
      "============================================================\n",
      "\n",
      "Chunk ID : chunk_53_0\n",
      "Metadata : {'Header_2': 'Conduct during travel', 'source': 'travel_and_reimbursement_policy.pdf'}\n",
      "Content  :\n",
      "## Conduct during travel  \n",
      "As outlined in the University Student Handbook, all students taking part in University-sponsored activities or trips are responsible for adhering to the policies and procedures included in the Student Handbook and other official  documents  and  publications.    Actions  detrimental  to  the  mission  of  the  University  and  to  the legitimate activities of the academic community are in violation of the University policy and will be subject to the University's student conduct process.\n",
      "============================================================\n",
      "\n",
      "Chunk ID : chunk_48_3\n",
      "Metadata : {'Header_2': '2. Documentation &amp; Records', 'source': 'travel_and_reimbursement_policy.pdf'}\n",
      "Content  :\n",
      "- name and address of overnight accommodations\n",
      "- costs to be paid by participants\n",
      "- a packing list including recommendations for clothing\n",
      "- the names and contact information of University representatives traveling with the students\n",
      "- any other pertinent information  \n",
      "If  traveling to different locations, the travel supervisor must provide separate lists of participants by destination/location  to  clarify  for  the  University  who  is  traveling  to  each  location  and  who  can  be contacted in the event of an emergency at each.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_samples = 5\n",
    "\n",
    "sample_size = min(num_samples, len(chunks))\n",
    "random_indices = random.sample(range(len(chunks)), sample_size)\n",
    "\n",
    "print(f\"--- Inspecting {sample_size} Random Chunks ---\\n\")\n",
    "\n",
    "for idx in random_indices:\n",
    "    print(f\"Chunk ID : {ids[idx]}\")\n",
    "    print(f\"Metadata : {metadatas[idx]}\")\n",
    "    print(f\"Content  :\\n{chunks[idx]}\")\n",
    "    print(\"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed7d0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_output_path = \"data/raw_pdfs/travel_policy_inspected.md\"\n",
    "\n",
    "with open(md_output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(markdown_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df0eab",
   "metadata": {},
   "source": [
    "## Generating embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99a80e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n",
      "Upserting into ChromaDB...\n",
      "Database populated successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating embeddings...\")\n",
    "# Generate vectors for all chunks\n",
    "embeddings = embedding_model.encode(chunks).tolist()\n",
    "\n",
    "print(\"Upserting into ChromaDB...\")\n",
    "collection.upsert(\n",
    "    documents=chunks,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    ")\n",
    "print(\"Database populated successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langraph_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
